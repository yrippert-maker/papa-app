name: smoke-docker

concurrency:
  # Prevent parallel runs of the same smoke workflow from racing
  # on the same nightly issue (create/comment/escalate).
  group: nightly-smoke-${{ github.workflow }}
  cancel-in-progress: false

permissions:
  id-token: write
  contents: read
  issues: write
  actions: read

on:
  pull_request:
    paths:
      - Dockerfile
      - .dockerignore
      - docker-compose.yml
      - scripts/smoke-docker.sh
      - package.json
      - package-lock.json
      - next.config.*
      - env.example
      - middleware.ts
      - app/**
      - components/**
      - lib/**
      - config/**
      - prisma/**
      - migrations/**
      - public/**
      - scripts/**
      - electron/**
      - templates/**
      - schemas/**
      - types/**
      - services/**
      - apps/**
      - .github/workflows/**
      - terraform/**
  push:
    branches: [main, master]
    # Без paths: main всегда проверяется полностью (страховка от забытых файлов)
  schedule:
    - cron: "15 5 * * *"  # 08:15 MSK (nightly)
  workflow_dispatch:
    # Test-only inputs for validating nightly-notify logic (FAIL x3 → CRITICAL → recovery)
    # Safe to keep in repo: inputs are used ONLY for manual workflow_dispatch
    # and do not affect scheduled nightly runs.
    inputs:
      force_fail:
        type: boolean
        required: false
        default: false
      force_is_schedule:
        type: boolean
        required: false
        default: true

jobs:
  smoke:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - uses: actions/checkout@v4

      - name: Docker smoke (prod image)
        env:
          DOCKER_BUILDKIT: "1"
        run: ./scripts/smoke-docker.sh

      - name: Dump diagnostics on failure
        if: failure()
        run: |
          mkdir -p /tmp/smoke-docker-artifacts
          echo "=== listeners (3001) ===" > /tmp/smoke-docker-artifacts/summary.txt
          lsof -nP -iTCP:3001 -sTCP:LISTEN >> /tmp/smoke-docker-artifacts/summary.txt 2>&1 || true
          echo -e "\n=== curl -v /api/health ===" >> /tmp/smoke-docker-artifacts/summary.txt
          curl -v http://127.0.0.1:3001/api/health >> /tmp/smoke-docker-artifacts/summary.txt 2>&1 || true
          echo -e "\n=== env (filtered) ===" >> /tmp/smoke-docker-artifacts/summary.txt
          (printenv | grep -E '^(PORT|NEXTAUTH|WORKSPACE|PAPA|AGENT|DOCKER)=' || true) >> /tmp/smoke-docker-artifacts/summary.txt

      - name: Upload smoke-docker artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: smoke-docker-artifacts
          path: /tmp/smoke-docker-artifacts
          if-no-files-found: ignore

      - name: Force failure (test only)
        # Test hook: allows manual simulation of smoke failure via workflow_dispatch.
        # Never runs on schedule; guarded by event_name == 'workflow_dispatch'.
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.force_fail }}
        run: exit 1

  nightly_notify:
    needs: [smoke]
    uses: ./.github/workflows/_lib/nightly-notify.yml
    with:
      workflow_name: smoke-docker
      smoke_result: ${{ needs.smoke.result }}
      # Treat workflow_dispatch as "schedule" ONLY when explicitly requested.
      # This enables safe manual testing of escalation/recovery logic
      # without modifying the reusable workflow.
      is_schedule: ${{ github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && inputs.force_is_schedule) }}
      severity_threshold: 3
      enable_slack: true
      enable_pagerduty: false
      auto_close_on_success: true
    secrets:
      AWS_GITHUB_SNS_PUBLISH_ROLE_ARN: ${{ secrets.AWS_GITHUB_SNS_PUBLISH_ROLE_ARN }}
      AWS_SNS_TOPIC_HIGH_ARN: ${{ secrets.AWS_SNS_TOPIC_HIGH_ARN }}
      AWS_SNS_TOPIC_CRITICAL_ARN: ${{ secrets.AWS_SNS_TOPIC_CRITICAL_ARN }}
      SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      PAGERDUTY_ROUTING_KEY: ${{ secrets.PAGERDUTY_ROUTING_KEY }}
